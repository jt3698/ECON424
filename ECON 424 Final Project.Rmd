---
title: "ECON 424 - Final Project (Winter 2023)"
author: "Submitted by Raphael Shawn Gozali (20805288) and Jason Tedjosoesilo (20801292)"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy = TRUE)
```



## Questions

For this project, these are the questions that we will be answering:

1. Without any interactions, which covariate(s) affect revenue the most? Are there any certain genres, actors, or keywords that affect revenue the most?

2. If we do interactions between genres, actors, and keywords, are there any particular combination(s) that affect revenue the most?

3. Do people’s preferences on movie genres and actors change over the decades?

## Data

In this project, we will be using the Movies dataset from Kaggle: https://www.kaggle.com/datasets/akshaypawar7/millions-of-movies?resource=download. We downloaded this dataset on April 2, 2023 on 2pm EST and continue to work with it locally. This dataset consists of around 723 thousand rows of movies, with their descriptions and details separated into columns. The response variable that we are focusing on from this dataset is the “revenue” column, which shows us the revenue of the movie. The covariates that might be useful from this dataset includes: date published, budget, genres, casts, keywords, and runtime.

```{r}
data <- read.csv("./movies.csv")
```

Before we do some analysis, we will clean the data. First, we will remove unnecessary columns and also data with zero revenues. We are only keeping released movies above 40 minutes as we are not including short movies in the data. This is based on the definition from the Academy of Motion Picture Arts and Sciences, which define a short film as "an original motion picture that has a running time of 40 minutes or less, including all credits". We also do not include movies with a runtime of value 999. There might also be duplicates in the data, so we need to remove duplicates as well. Since people can add random movies to this database, we try as best as we can to filter those out. To make sure that a movie is legitimate, we filter the movies that do not have any genres, production companies, and credits (all three are empty values).

```{r}
# Only movies that has revenue, is released, and more than 40 minutes long
# Also remove if runtime = 999 and remove the movies with missing genres, production companies, and credits
clean_data <- data[data$revenue > 0 & data$status == "Released" & data$runtime > 40 & data$runtime != 999 & !(data$genres == "" & data$production_companies == "" & data$credits == ""), !names(data) %in% c("overview", "popularity","status", "tagline", "vote_average","vote_count","poster_path","backdrop_path","recommendations")]


# removing empty ID rows
clean_data <- clean_data[!is.na(clean_data$id),]

#resetting row.names
row.names(clean_data) <- NULL

#remove duplicate data
clean_data <- clean_data[!duplicated(clean_data[,1]),]

write.csv(clean_data, file="./movies_filtered.csv", row.names = FALSE)

n <- length(clean_data[,1])

```

We took this data and use a Python API to calculate the adjusted revenues by adding inflation factors. Here are the Python code below.

```{r, eval=FALSE}
# import libraries
import pandas as pd
import matplotlib.pyplot as plt
import requests
import json
from tqdm import tqdm
from pathlib import Path 

# get movies from movies_filtered.csv
movies = pd.read_csv(r'movies_filtered.csv')
n = len(movies)

# Function to get the inflation rate from the API
# Input: start_time (based on release_date)
# Output: inflation rate (inflated to April 3, 2023)
def get_dollar(start_time):
    api_url = "https://www.statbureau.org/calculate-inflation-price-jsonp?jsoncallback=?"

    headers = {'Content-type': 'application/json'}

    payload = {
        "country": "united-states",
        "start": str(start_time),
        "end": "2023/04/03",
        "amount": "1",
        "format": True
    }
    response = requests.post(api_url,  data=json.dumps(payload), headers=headers)
    my_bytes = response.content
    
    amount_s = my_bytes.decode('utf8').replace("'", '"')
    amount_s = amount_s[4:-2]
    return float(amount_s)

# getting the inflation rate of each movie
inflation = [0] * n
failed = []
for i in tqdm(range(n)):
    try:
        inflation[i] = get_dollar(movies.iloc[i]['release_date'])
    except:
        failed.append(i)

# data with no release_date will be given inflation = 1 (no inflation)
for fail in failed:
    inflation[fail] = 1

# getting the adjusted revenue for each movie    
revenue_adjusted_c = movies['revenue'] * np.array(inflation)
df2 = movies.assign(revenue_adjusted=revenue_adjusted_c)

# exporting the file movies_adjusted.csv
filepath = Path('movies_adjusted.csv')  
filepath.parent.mkdir(parents=True, exist_ok=True)  
df2.to_csv(filepath)
```

We calculated all the revenues inflated to April 3, 2023 and include them in the data as a new column called "revenue_adjusted". The data that do not have release_date will have their adjusted revenue be the same as their revenue.

```{r, fig.width = 6, fig.height = 4}
clean_data <- read.csv("./movies_adjusted.csv")
n <- length(clean_data[,1])

hist(log(clean_data$revenue_adjusted), 
     breaks = quantile(log(clean_data$revenue_adjusted), p = seq(0,1 , length.out = 21)), 
     freq = FALSE, xlab = "log(revenue_adjusted)", main = "Histogram of log(revenue_adjusted)")
```

From the histogram above, we can see that when we split the dataset into 20 bins of 5% quantiles each, most of them have high values on revenues after being adjusted to inflation.

After cleaning the data, we have a total of 14,678 rows of movies.

## Limitations

Before we start with our methods, we would like to address the limitation of the methods that we are using here.

```{r}
clean_data$year <- format(as.Date(clean_data$release_date), format = "%Y")
clean_data$decades <- floor(as.numeric(clean_data$year) / 10) * 10
df_list <- split(clean_data, clean_data$decades)
```

```{r, fig.height=4, fig.width=10}

par(mfrow = c(1,2))
hist(clean_data$decades, xlab = "Decades",
     main = "Distribution of Movies by Decades")
plot(as.Date(clean_data$release_date), log(clean_data$revenue_adjusted), 
     xlim = c(as.Date("1910-01-01"), as.Date("2025-01-01")), 
     col="blue", lwd = 0.5, xlab = "Release Date", ylab = "Adjusted Revenue",
     main = "Release Date vs. Adjusted Revenue")
```

Based on the histogram above, we can see that the distribution of the movies is not equal across time. There are more recent movies in the dataset than older movies. Also, from the scatterplot above, it is visible that the variance of the data changes over time. This shows that our data is non-stationary. In addition, we do not see a lot of data points in the early decades, and not a lot of them have low revenues. Since this is a problem, we do not use time as a variable in our model. We do not change the distribution of the data since it is better to have more data on recent decades. This gives us a notion of weights on the data, with more movies closer to the present.

Other than that, note that the data cleaning process is not perfect. Since the data source is publicly available to be added, there are some made-up data points in the raw data. We cleaned it as best as we could, but there is no guarantee on whether there are still some made-up data in this dataset.

## Methods & Results

To answer the questions above, we will create some models using LASSO method via cv.glmnet function. We use LASSO since we will have a lot of variables coming from the text columns. Thus, we use LASSO to do variable reduction to obtain a balanced model. 

### Model without interactions

To build the model, we will have to do a text analysis on the texts first. There are five columns which have text variables: production_companies, genres, credits, keywords, and original_language. The terms that we are using from these columns are separated by dashes ("-"). Using this information, we will separate these text columns into a sparse matrix full of terms. For each document, the term will be equal to 1 if it appears on that document, and 0 if it does not appear. After getting each term separated, we added single-letter prefixes with "$" on the terms to be able to tell which column the terms come from. Then, we filter the sparse matrix so that only terms that appear in at least 50 movies are included to be in the model. 

```{r}
library(pdftools)
library(tm)
library(SnowballC)

to_another <- content_transformer(function(x, y, z) gsub(y, z, x))

add_pre <- function(x, pre) {
  ifelse(!is.na(x) & x != "" & nchar(x) > 0 & x != " ", return(paste(pre, x, sep="")), return(x))
}

add_prefix <- content_transformer(add_pre)
```

For terms coming from the production_companies column, we have to change "Metro-Goldwyn-Mayer" to "Metro_Goldwyn_Mayer" due to the name having dashes in it. This is the only company that appears in at least 50 movies that contain "-" in its name. We added "p$" to indicate production_companies terms. From here, we got a result of 71 production companies.

```{r}

#### PRODUCTION COMPANIES
prod_comp <- clean_data$production_companies
docs <- Corpus(VectorSource(prod_comp))
# Remove "-" from the name since it is the separator of the data
docs <- tm_map(docs, to_another, "Metro-Goldwyn-Mayer", "Metro_Goldwyn_Mayer")
docs <- tm_map(docs, to_another, "no_production_companies", "")
docs <- tm_map(docs, to_another, " ", "_")
docs <- tm_map(docs, to_another, "-", " ")
docs <- tm_map(docs, stripWhitespace)
# adding prefix p$ for production_companies
docs <- tm_map(docs, add_prefix, "p$")
docs <- tm_map(docs, to_another, " ", " p$")
dtm <- DocumentTermMatrix(docs)

#get production companies in at least 50 movies
key_pc <- sort(findFreqTerms(dtm, 50))
key_pc
```

For terms coming from genres column, we added "g$" to indicate genre terms. We decided to use all genres since there are only 19 of them. The only genre with less than 50 movies is "tv_movie", but we decided to include it as a term.

```{r}
#### GENRES
genres <- clean_data$genres
docs2 <- Corpus(VectorSource(genres))
docs2 <- tm_map(docs2, to_another, " ", "_")
docs2 <- tm_map(docs2, to_another, "-", " ")
docs2 <- tm_map(docs2, stripWhitespace)
# adding prefix g$ for genres
docs2 <- tm_map(docs2, add_prefix, "g$")
docs2 <- tm_map(docs2, to_another, " ", " g$")
dtm2 <- DocumentTermMatrix(docs2)

#get all genres
key_gen <- sort(findFreqTerms(dtm2, 0))
key_gen
```

For terms coming from the credits column, there are a lot of names that contain a dash symbol. The names of Korean casts can be dealt with since they follow a regex pattern, as seen in the code. However, this does not cover all names. It is hard to clean this, so we decided to remove one-word names from the results. We added "c$" to indicate cast terms. From this column, we managed to get 67 terms.

```{r}

#### CREDITS
casts <- clean_data$credits
docs3 <- Corpus(VectorSource(casts))
docs3 <- tm_map(docs3, to_another, " ", "_")
# deal with Korean names
docs3 <- tm_map(docs3, to_another, "_([[:alpha:]]+)-([[:lower:]]+)$", "_\\1_\\2")
docs3 <- tm_map(docs3, to_another, "_([[:alpha:]]+)-([[:lower:]]+)-", "_\\1_\\2-")
docs3 <- tm_map(docs3, to_another, "-", " ")
docs3 <- tm_map(docs3, stripWhitespace)
# adding prefix c$ for casts
docs3 <- tm_map(docs3, add_prefix, "c$")
docs3 <- tm_map(docs3, to_another, " ", " c$")
dtm3 <- DocumentTermMatrix(docs3)

#get all casts in at least 50 movies
key_cast <- sort(findFreqTerms(dtm3, 50))

#hard to remove "-" from two-worded names separated by "-"
#so we remove single-word names that comes from them
key_cast <- key_cast[grepl('_', key_cast)]
key_cast
```

For terms coming from the keywords column, we added "k$" to indicate keyword terms. From this column, we managed to get 312 terms.

```{r}
#### KEYWORDS
keywords <- clean_data$keywords
docs4 <- Corpus(VectorSource(keywords))
docs4 <- tm_map(docs4, to_another, " ", "_")
docs4 <- tm_map(docs4, to_another, "-", " ")
docs4 <- tm_map(docs4, stripWhitespace)
# adding prefix k$ for keywords
docs4 <- tm_map(docs4, add_prefix, "k$")
docs4 <- tm_map(docs4, to_another, " ", " k$")
dtm4 <- DocumentTermMatrix(docs4)

#get all keywords in at least 50 movies
key_keys <- sort(findFreqTerms(dtm4, 50))
key_keys
```

For terms coming from the original_language column, we added "l$" to indicate language terms. From this column, we managed to get 20 terms.

```{r}
#### ORIGINAL LANGUAGE
og_lng <- clean_data$original_language
docs5 <- Corpus(VectorSource(og_lng))
# adding prefix l$ for language
docs5 <- tm_map(docs5, add_prefix, "l$")
docs5 <- tm_map(docs5, to_another, " ", " l$")
dtm5 <- DocumentTermMatrix(docs5)

#get all languages in at least 50 movies
key_lang <- sort(findFreqTerms(dtm5, 50))
key_lang
```

With all of the terms above combined, we managed to get a total of 489 terms. We used these terms as variables in our model, and include the budget_adjusted as a variable as well. Due to the large values of budgets and revenues, we decided to use log transformation on both of these variables and do modeling with them.

```{r}
X <- cbind(dtm[, key_pc], dtm2[, key_gen], 
           dtm3[, key_cast], dtm4[, key_keys], 
           dtm5[, key_lang])
y_adj <- log(clean_data$revenue_adjusted)
```


Originally, we would have liked to include the budget column in the model. However, it turns out that 5,178 rows have either zero budget or an unspecified amount. So, we decided to not include this column in the model.

```{r}
library(glmnet)
set.seed(424)
model1 <- cv.glmnet(as.matrix(X), y_adj)
plot(model1, xvar = "lambda")
log(model1$lambda.min)
```

From the plot above, we can see that based on the cross-validation, the lowest MSE is reached when log lambda is -4.851949. 

```{r}
coef1 <- coef(model1, s = "lambda.min")
length(coef1[which(coef1 != 0),][-1]) # -1 to exclude intercept
# to see all non-zero coefficients
# sort(coef1[which(coef1 != 0),1], decreasing = TRUE)
```

From 489 variables, we have 429 variables in our model. Below is the intercept of the model, the top 10 variables that positively affect revenue, and the top 10 variables that negatively affect revenue.

```{r}
ic1 <- coef1[c("(Intercept)"),1] 
paste("The intercept is ", ic1)
coef1_sort <- sort(coef1[,1], decreasing = TRUE)[-1]
# Top 10 variables that positively affect the revenue
head(coef1_sort, 10)
#Top 10 variables that negatively affect the revenue
tail(coef1_sort, 10)
```

From the result above, we can see that the top variables with a positive correlation toward revenue are all production companies. These production companies are all big companies or subsidiaries of them. For example, Walt Disney Studios is one of the biggest animation production companies. Screen Gems and Columbia Pictures are both subsidiaries of Sony Pictures, which is a really big production company.

On the other hand, the top variables with negative correlation towards the revenue come from diverse terms. For instance, one of the genres there is 'documentary'. The number of people who are interested in watching documentary movies is probably not large. This might be why documentary movies might not get a lot of revenue. There are also several language terms (Urdu, Farsi, and Estonian). It looks like the movies with these languages are not popular and thus the revenue is low.

In order to interpret this model, we need to transform the revenue back from the log transformation. Thus, all coefficients will become exponents with base e, and these variables will become multipliers to the intercept in the model. For example, the movies with Screen Gems will generate $e^{2.691183} = 14.74911$ times more than the movies without Screen Gems, with all the other variables unchanged. If the movie does not contain any of the terms in the model, the movie is expected to have a revenue based on the intercept, which is $e^{12.6124353194507} = 300269.4$ US dollars.

### Model with interactions

To improve the model more, we want to include interactions between terms in the model. Since it is too much to interact with all of the terms that we have, we will get the top 10 terms from each column and interact with them with each other to make a pair of terms. We do not include the original_language column in the interaction since most of the movies' original language is English. So, we have 40 terms to be paired with each other, giving us 780 interaction variables.

```{r}
# top 10 of each
key_pc2 <- names(findMostFreqTerms(dtm, 10, INDEX = rep(1, each = n))[[1]])
key_gen2 <- names(findMostFreqTerms(dtm2, 10, INDEX = rep(1, each = n))[[1]])
key_cast2 <- names(findMostFreqTerms(dtm3, 14, INDEX = rep(1, each = n))[[1]])
key_cast2 <- key_cast2[grepl('_', key_cast2)] # 4 of them are single names
key_keys2 <- names(findMostFreqTerms(dtm4, 10, INDEX = rep(1, each = n))[[1]])
int_vars <- c(key_pc2, key_gen2, key_cast2, key_keys2)
inact <- c()
inact_name <- c()
for(i in 1:(length(int_vars)-1)){
  for (j in (i+1):length(int_vars)){
    a = as.matrix(X[,int_vars[i]])
    b = as.matrix(X[,int_vars[j]])
    var_name = paste(int_vars[i],".",int_vars[j])
    v = a*b
    inact <- cbind(inact, v)
    inact_name <- c(inact_name, var_name)
  }
}

df_inact = data.frame(inact)
colnames(df_inact) <- inact_name
```

We add these 780 variables on top of the initial 489 variables in the first model, giving us 1269 variables for the second model.

```{r}
X2 <- cbind(dtm[, key_pc], dtm2[, key_gen], 
            dtm3[, key_cast], dtm4[, key_keys], 
            dtm5[, key_lang], df_inact)
model2 <- cv.glmnet(as.matrix(X2), y_adj)
plot(model2, xvar = "lambda")
log(model2$lambda.min)
```

From the plot above, we can see that based on the cross-validation, the lowest MSE is reached when log lambda is -4.479814.

```{r}
coef2 <- coef(model2, s = "lambda.min")
length(coef2[which(coef2 != 0),][-1]) # -1 to exclude intercept

# to see all the non-zero coefficients
# sort(coef2[which(coef2 != 0),1], decreasing = TRUE)
```

From 1269 variables, we have 710 variables in our model with interactions. Below is the intercept of the model, the top 10 variables that positively affect revenue, and the top 10 variables that negatively affect revenue.

```{r}
ic2 <- coef2[c("(Intercept)"),1]
paste("The intercept is ", ic2)
paste("Top 10 variables that positively affect the revenue:")
coef2_sort <- sort(coef2[,1], decreasing = TRUE)[-1]
head(coef2_sort, 10)
paste("Top 10 variables that negatively affects the revenue:")
tail(coef2_sort, 10)
```

From the results above, we notice that the top variables have changed from the first model. The top variables with positive correlation towards the revenue are mostly still production companies, except for one. This interaction between a cast "Robert De Niro" and a keyword "Woman Director" is really interesting. From our research on the internet, turns out that Robert De Niro have only worked with 4 female directors before, producing a total of 4 movies. The most popular movie from this interaction being "The Intern", with a revenue of nearly 200 million dollars. This model suggests that Robert De Niro should be a part of more projects with female directors.

On the other hand, there are a lot of interaction variables among the top variables with a negative correlation to the revenue. One of the interactions is the cast "Nicolas Cage" and "Willem Dafoe". This is surprising because both actors are really popular. One of the movies that they played together is "Dog Eat Dog" which only gathers $80 of revenue based on the dataset. This suggests that the movies that they were both cast in performed relatively badly compared to movies where they are performing without each other.

### Models with data separated into decades

Next, we will try to see which terms are included in the model (without interactions) when we split the data set into decades.

```{r}

for (df in df_list){
  #### PRODUCTION COMPANIES
  sub_prod_comp <- df$production_companies
  sub_docs <- Corpus(VectorSource(sub_prod_comp))
  # Remove "-" from the name since it is the splitter symbol of the data
  sub_docs <- tm_map(sub_docs, to_another, "Metro-Goldwyn-Mayer", "Metro_Goldwyn_Mayer")
  sub_docs <- tm_map(sub_docs, to_another, "no_production_companies", "")
  sub_docs <- tm_map(sub_docs, to_another, " ", "_")
  sub_docs <- tm_map(sub_docs, to_another, "-", " ")
  sub_docs <- tm_map(sub_docs, stripWhitespace)
  # adding prefix p$ for production_companies
  sub_docs <- tm_map(sub_docs, add_prefix, "p$")
  sub_docs <- tm_map(sub_docs, to_another, " ", " p$")
  sub_dtm <- DocumentTermMatrix(sub_docs)

  #get top 10 production companies
  sub_key_pc <- findMostFreqTerms(sub_dtm, 10, INDEX = rep(1, each = length(df[,1])))
  #print(sub_key_pc)
  
  #### GENRES
  sub_genres <- df$genres
  sub_docs2 <- Corpus(VectorSource(sub_genres))
  sub_docs2 <- tm_map(sub_docs2, to_another, " ", "_")
  sub_docs2 <- tm_map(sub_docs2, to_another, "-", " ")
  sub_docs2 <- tm_map(sub_docs2, stripWhitespace)
  # adding prefix g$ for genres
  sub_docs2 <- tm_map(sub_docs2, add_prefix, "g$")
  sub_docs2 <- tm_map(sub_docs2, to_another, " ", " g$")
  sub_dtm2 <- DocumentTermMatrix(sub_docs2)

  #get top 10 genres
  sub_key_gen <- findMostFreqTerms(sub_dtm2, 10, INDEX = rep(1, each = length(df[,1])))
  #sub_key_gen
  
  #### CREDITS
  sub_casts <- df$credits
  sub_docs3 <- Corpus(VectorSource(sub_casts))
  sub_docs3 <- tm_map(sub_docs3, to_another, " ", "_")
  # deal with Korean names
  sub_docs3 <- tm_map(sub_docs3, to_another, "_([[:alpha:]]+)-([[:lower:]]+)$", "_\\1_\\2")
  sub_docs3 <- tm_map(sub_docs3, to_another, "_([[:alpha:]]+)-([[:lower:]]+)-", "_\\1_\\2-")
  sub_docs3 <- tm_map(sub_docs3, to_another, "-", " ")
  sub_docs3 <- tm_map(sub_docs3, stripWhitespace)
  # adding prefix c$ for casts
  sub_docs3 <- tm_map(sub_docs3, add_prefix, "c$")
  sub_docs3 <- tm_map(sub_docs3, to_another, " ", " c$")
  sub_dtm3 <- DocumentTermMatrix(sub_docs3)

  #get top 10 casts
  sub_key_cast <- findMostFreqTerms(sub_dtm3, 10, INDEX = rep(1, each = length(df[,1])))
  
  #### KEYWORDS
  sub_keywords <- df$keywords
  sub_docs4 <- Corpus(VectorSource(sub_keywords))
  sub_docs4 <- tm_map(sub_docs4, to_another, " ", "_")
  sub_docs4 <- tm_map(sub_docs4, to_another, "-", " ")
  sub_docs4 <- tm_map(sub_docs4, stripWhitespace)
  # adding prefix k$ for keywords
  sub_docs4 <- tm_map(sub_docs4, add_prefix, "k$")
  sub_docs4 <- tm_map(sub_docs4, to_another, " ", " k$")
  sub_dtm4 <- DocumentTermMatrix(sub_docs4)

  #get top 10 keywords
  sub_key_keys <- findMostFreqTerms(sub_dtm4, 10, INDEX = rep(1, each = length(df[,1])))
  #sub_key_keys
  
  #### ORIGINAL LANGUAGE
  sub_og_lng <- df$original_language
  sub_docs5 <- Corpus(VectorSource(sub_og_lng))
  # adding prefix l$ for language
  sub_docs5 <- tm_map(sub_docs5, add_prefix, "l$")
  sub_docs5 <- tm_map(sub_docs5, to_another, " ", " l$")
  sub_dtm5 <- DocumentTermMatrix(sub_docs5)

  #get all languages in at least 50 movies
  sub_key_lang <- findMostFreqTerms(sub_dtm5, 10, INDEX = rep(1, each = length(df[,1])))
  #sub_key_lang
  
  sub_X <- cbind(sub_dtm[, names(sub_key_pc[[1]])], sub_dtm2[, names(sub_key_gen[[1]])], sub_dtm3[, names(sub_key_cast[[1]])], sub_dtm4[, names(sub_key_keys[[1]])], sub_dtm5[, names(sub_key_lang[[1]])])
  sub_y <- log(df$revenue_adjusted)
  sub_model1 <- cv.glmnet(cbind(as.matrix(sub_X), budget = log(df$budget)), sub_y)
  sub_coef1 <- coef(sub_model1, s = "lambda.min")
  sub_coef_sort <- sort(sub_coef1[which(sub_coef1 != 0),1], decreasing = TRUE)
  
  print(df$decades[1])
  print(sub_coef_sort)
  
}

```

#### 1920s
We see that in the 1920s, the strongest indicators that a movie is successful are 'Music', 'World War 1', 'Based on novel or book' 'Charles Chaplin Productions'. Movies about World War 1 is a particularly interesting one because it would be then that movies were being made about the war that just ended about a decade prior, and it was a popular sub-genre.

#### 1930s
We see that in the 1930s, movies with Irving Bacon were very popular. Irving Bacon appeared in 500 movies throughout his life but he was most active in the 1930s and 1940s. The model seems to suggest that he was in a lot of movies that generated high revenue.

#### 1940s
We see that in the 1940s, it seems that family movies were particularly favored in this decade. We also have our first notice of Disney as an indicator. The 1940s were a part of the Golden Age of Disney and their movies were performing well.

#### 1950s
Once again we see that Disney movies continued to be popular. Movies that told epic stories were also popular. We noticed the movie Ben-Hur which was released in 1959 as a particularly strong example of this. Adjusted for inflation, the movie has made approximately 180 million USD at the box office.

#### 1960s
We can see varying popular indicators in the 1960s. We note a few notable ones here. Western movies were popular and John Wayne who was essentially an icon of the genre was also in movies that were very successful. Japanese movies were also increasing in popularity in that decade.

#### 1970s
In the 1970s, most of the top indicators of a movie's success were the production companies producing the movies.

#### 1980s
The 1980s continued the popularity of big production companies. It was also a good decade for Sylvester Stallone as there were several Rocky movies that performed particularly well.

#### 1990s - 2020s
We see that the dominating indicator for these decades were mostly production companies.

## Summary

From our project, we see that the model without interactions shows that most production companies affect the revenue positively, while some of the non-popular languages and genres affect the revenue negatively. On the other hand, in our analysis of the model with interactions, we could not find a satisfactory generalized conclusion. However, we did find a few interesting interactions which suggested some aspects that are open for moviemakers to interpret and discuss further. 

From the analysis of the different decades above, we see that people's preferences mostly change over time. However, in recent times we see that big production companies have been consistently performing well at the box office. Another interesting indicator we notice is that movies that were based on a novel or book have consistently been a positive indicator even since the 1920s.

